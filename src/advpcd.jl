"""
    advpcd!(rbm, data; q, Q, ...)

Trains the RBM on data using Persistent Contrastive divergence with constraints.
Matrix `q` contains the 1st-order constraints, that `q[...,t]' * W` be small, for each `t`.
Matrix `Q` contains the 2nd-order constraints, that `W' * Q[...,t] * W` be small, for each `t`.
"""
function advpcd!(
    rbm::RBM,
    data::AbstractArray;
    batchsize::Int = 1,
    epochs::Int = 1,
    wts::Union{AbstractVector, Nothing} = nothing,
    steps::Int = 1, # fantasy chains MC steps
    optim = default_optimizer(_nobs(data), batchsize, epochs),
    moments = moments_from_samples(rbm.visible, data; wts), # sufficient statistics for visible layer

    # regularization
    l2_fields::Real = 0, # visible fields L2 regularization
    l1_weights::Real = 0, # weights L1 regularization
    l2_weights::Real = 0, # weights L2 regularization
    l2l1_weights::Real = 0, # weights L2/L1 regularization

    # gauge
    zerosum::Bool = true, # zerosum gauge for Potts layers
    rescale::Bool = true, # scale hidden unit activations to var(h) = 1
    center::Bool = true, # center gradients

    # damping for hidden activity statistics tracking
    œÅh::Real = 99//100,
    œµh = 1e-2, # prevent vanishing var(h)

    callback = nothing, # called for every batch
    mode::Symbol = :pcd, # :pcd, :cd, or :exact

    vm = fantasy_init(rbm.visible; batchsize, mode), # fantasy chains
    shuffle::Bool = true,

    # constraints are given as a list, where each entry describes the constraints applied
    # to a group of hidden units (the groups must be exclusive)
    # For Potts units, q, Q should themselves be zerosum!
    qs::AbstractVector{<:AbstractArray{<:Real}} = default_qs(rbm), # 1st-order constraints
    Qs::AbstractVector{<:AbstractArray{<:Real}} = default_Qs(rbm, qs), # 2nd-order constraints
    ŒªQ::Real = 0, # 2nd-order adversarial soft constraint, penalty
    # indices of constrained hidden units in each group (the groups must not intersect)
    ‚Ñãs::AbstractVector{<:CartesianIndices} = default_‚Ñãs(rbm, qs)
)
    @assert size(data) == (size(rbm.visible)..., size(data)[end])
    @assert isnothing(wts) || _nobs(data) == _nobs(wts)
    @assert œµh ‚â• 0

    @assert 0 ‚â§ ŒªQ < Inf # hard 2nd-order constraint not supported
    @assert length(qs) == length(Qs) == length(‚Ñãs)
    @assert empty_intersections(‚Ñãs)

    # indices in visible dimensions
    ùí± = CartesianIndices(size(rbm.visible))

    # we center units using their average activities
    ave_v = batchmean(rbm.visible, data; wts)
    ave_h, var_h = total_meanvar_from_inputs(rbm.hidden, inputs_h_from_v(rbm, data); wts)
    @assert all(var_h .+ œµh .> 0)

    # gauge constraints
    zerosum && zerosum!(rbm)
    rescale && rescale_hidden!(rbm, sqrt.(var_h .+ œµh))

    wts_mean = isnothing(wts) ? 1 : mean(wts)

    # impose 1st-order constraint on initial weights
    for (q, ‚Ñã) in zip(qs, ‚Ñãs)
        rbm.w[ùí±, ‚Ñã] .= kernelproj(rbm.w[ùí±, ‚Ñã], q) # 1st-order constraint is hard
    end

    for epoch in 1:epochs, (batch_idx, (vd, wd)) in enumerate(minibatches(data, wts; batchsize, shuffle))
        ‚àÇd = ‚àÇfree_energy(rbm, vd; wts = wd, moments)
        ‚àÇm = ‚àÇlogpartition(rbm; vd, vm, wd, mode, steps)
        ‚àÇ = ‚àÇd - ‚àÇm

        batch_weight = isnothing(wts) ? 1 : mean(wd) / wts_mean
        ‚àÇ *= batch_weight

        ave_h_batch = grad2ave(rbm.hidden, -‚àÇd.hidden)
        var_h_batch = grad2var(rbm.hidden, -‚àÇd.hidden)
        œÅh_eff = œÅh ^ batch_weight
        ave_h .= œÅh_eff * ave_h .+ (1 - œÅh_eff) * ave_h_batch
        var_h .= œÅh_eff * var_h .+ (1 - œÅh_eff) * var_h_batch
        @assert all(var_h .+ œµh .> 0)

        # regularize
        ‚àÇregularize!(‚àÇ, rbm; l2_fields, l1_weights, l2_weights, l2l1_weights, zerosum)

        if 0 < ŒªQ < Inf
            for (Q, ‚Ñã) in zip(Qs, ‚Ñãs)
                ‚àÇ.w[ùí±, ‚Ñã] .+= ŒªQ .* ‚àÇwQw(rbm.w[ùí±, ‚Ñã], Q)
            end
        end

        if center
            ‚àÇ = center_gradient(rbm, ‚àÇ, ave_v, ave_h)
        end

        for (q, ‚Ñã) in zip(qs, ‚Ñãs)
            # 1st-order constraint is hard
            ‚àÇ.w[ùí±, ‚Ñã] .= kernelproj(‚àÇ.w[ùí±, ‚Ñã], q)
        end

        # compute parameter update step, according to optimizer algorithm
        update!(‚àÇ, rbm, optim)

        if center # get step in uncentered parameters
            ‚àÇ = uncenter_step(rbm, ‚àÇ, ave_v, ave_h)
        end

        RBMs.update!(rbm, ‚àÇ)

        # respect gauge constraints
        zerosum && zerosum!(rbm)
        rescale && rescale_hidden!(rbm, sqrt.(var_h .+ œµh))

        for (q, ‚Ñã) in zip(qs, ‚Ñãs)
            #= Since the adaptive gradients update and
            the centering might move the weights towards q,
            we project the weights to be orthogonal to q after
            each update. =#
            rbm.w[ùí±, ‚Ñã] .= kernelproj(rbm.w[ùí±, ‚Ñã], q) # 1st-order constraint is hard
        end

        isnothing(callback) || callback(; rbm, ‚àÇ, optim, epoch, batch_idx, vm, vd, wd)
    end
    return rbm
end

function default_qs(rbm::RBM)
    q = similar(rbm.w, eltype(rbm.w), size(rbm.visible)..., 1)
    return empty([q])
end

function default_Qs(rbm::RBM, qs::AbstractVector{<:AbstractArray{<:Real}})
    return [Zeros{eltype(q)}(size(rbm.visible)..., size(rbm.visible)..., 1) for q in qs]
end

function default_‚Ñãs(rbm::RBM, qs::AbstractVector{<:AbstractArray{<:Real}})
    return [CartesianIndices(size(rbm.hidden)) for q in qs]
end
